{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_content(url):\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    \n",
    "    name_box = soup.findAll('p', attrs={'class': 'story-body-text story-content'})\n",
    "    content = [x.text for x in name_box]\n",
    "    content_final = ' '.join(content)\n",
    "    \n",
    "    if(content_final == ''):\n",
    "        name_box = soup.findAll('p')\n",
    "        content = [x.text for x in name_box]\n",
    "        content_final = ' '.join(content)\n",
    "\n",
    "    return(content_final)\n",
    "\n",
    "import pandas as pd\n",
    "content_data = pd.read_csv('data/content_data.csv')\n",
    "content_data['length'] = content_data['content'].str.strip().str.len()\n",
    "\n",
    "import gensim\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import string\n",
    "\n",
    "def pre_process(text):    \n",
    "    stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "    punctuation = set(string.punctuation)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokenizer = RegexpTokenizer(r'[0-9a-zA-Z]+')\n",
    "\n",
    "    def convert_tag(tag):\n",
    "        \"\"\"\n",
    "        Convert the tag given by nltk.pos_tag to the tag used by wordnet\n",
    "        \"\"\"\n",
    "        tag_dict = {'N': 'n', 'J': 'a', 'R': 'r', 'V': 'v'}\n",
    "        try:\n",
    "            return tag_dict[tag[0]]\n",
    "        except KeyError:\n",
    "            return 'a'\n",
    "\n",
    "    cl_text = (\" \").join(tokenizer.tokenize(text))\n",
    "    cl_text = (\" \").join([s for s in cl_text.lower().split() if s not in stopwords])\n",
    "    cl_text = (\"\").join([s for s in cl_text if s not in punctuation])\n",
    "    cl_text = nltk.word_tokenize(cl_text)\n",
    "    pos = nltk.pos_tag(cl_text)\n",
    "    pos = [convert_tag(t[1]) for t in pos]\n",
    "    cl_text = [lemmatizer.lemmatize(cl_text[i], pos[i]) for i in range(len(cl_text))]\n",
    "    return cl_text\n",
    "\n",
    "content_data['content_clean'] = content_data['content'].apply(pre_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/saketguddeti/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:1059: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "n_features = 1000\n",
    "n_top_words = 20\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.6, min_df=10,\n",
    "                                   max_features=n_features,\n",
    "                                   stop_words='english', \n",
    "                                   token_pattern='(?u)\\\\b\\\\w\\\\w\\\\w+\\\\b')\n",
    "\n",
    "tfidf = tfidf_vectorizer.fit_transform(content_data['content_clean'].apply(lambda x: (\" \").join(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 30\n",
    "\n",
    "nmf = NMF(n_components=num_topics, random_state = 1, alpha = 0.1, l1_ratio = 0.5).fit(tfidf)\n",
    "\n",
    "def topic_corpus():\n",
    "    doctopic = nmf.transform(tfidf)\n",
    "    doctopic = doctopic/np.sum(doctopic, axis = 1, keepdims = True)\n",
    "    doctopic = pd.DataFrame(doctopic)\n",
    "    return(doctopic)\n",
    "\n",
    "topic_corpus = topic_corpus()\n",
    "\n",
    "def query_article_topic(list):\n",
    "    query_content = []\n",
    "    for at in list:\n",
    "        try:\n",
    "            query_content.append(pre_process(extract_content(at)))\n",
    "        except:\n",
    "            query_content.append([''])\n",
    "    \n",
    "    query_content = pd.Series([(\" \").join(x) for x in query_content])\n",
    "    tfidf_query_content = tfidf_vectorizer.transform(query_content)\n",
    "    topic_dist = nmf.transform(tfidf_query_content)\n",
    "    topic_dist = topic_dist#/np.sum(topic_dist, axis = 1, keepdims = True)\n",
    "    topic_dist = pd.DataFrame(topic_dist)\n",
    "    return(topic_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "\n",
    "def pre_process_doc2vec(text):    \n",
    "    stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "    punctuation = set(string.punctuation).union(set(['“','”','—','’','‘']))\n",
    "    punctuation.remove('-')\n",
    "    cl_text = (\" \").join([s for s in text.lower().split() if s not in stopwords])\n",
    "    cl_text = (\"\").join([s for s in cl_text if s not in punctuation])\n",
    "    cl_text = nltk.word_tokenize(cl_text)\n",
    "    return cl_text\n",
    "\n",
    "# class LabeledLineSentence(object):\n",
    "#     def __init__(self, doc_list):\n",
    "#        self.doc_list = doc_list\n",
    "#     def __iter__(self):\n",
    "#         for idx, doc in enumerate(self.doc_list):\n",
    "#             yield LabeledSentence(words = pre_process_doc2vec(doc), tags = [idx])\n",
    "            \n",
    "# it = LabeledLineSentence(list(content_data['content']))\n",
    "\n",
    "# model = Doc2Vec(size=50, min_count=5, alpha=0.025, min_alpha=0.025, workers=8)\n",
    "# model.build_vocab(it)\n",
    "\n",
    "# for epoch in range(10):\n",
    "#     model.train(it, total_examples = model.corpus_count, epochs = model.iter)\n",
    "#     model.alpha -= 0.002  # decrease the learning rate\n",
    "#     model.min_alpha = model.alpha  # fix the learning rate, no decay\n",
    "    \n",
    "# model.save(\"data/doc2vec.model\")\n",
    "model = Doc2Vec.load(\"data/doc2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# from ipywidgets import widgets, HBox, VBox, Layout, Button, Label\n",
    "# from IPython.display import display, HTML, clear_output\n",
    "# import requests\n",
    "# import webbrowser\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import os\n",
    "\n",
    "# def reset_pref(sender):\n",
    "#     usr = text_user.value\n",
    "#     df = pd.read_csv('data/df.csv')\n",
    "#     df = df.loc[df['name'] != usr,]\n",
    "#     df.to_csv('data/df.csv', index = False)\n",
    "#     try:\n",
    "#         os.remove('data/user_pref/%s.csv' %usr)\n",
    "#     except:\n",
    "#         None\n",
    "\n",
    "# htmlscript_ipywidget_disable_closing = '''<script>\n",
    "# disable = true\n",
    "# function disable_ipyw_close(){\n",
    "#     if(disable){\n",
    "#         $('div.widget-area > div.prompt > button.close').hide()\n",
    "#     }\n",
    "#     else{\n",
    "#         $('div.widget-area > div.prompt > button.close').show()    \n",
    "#     }\n",
    "#     disable = !disable\n",
    "# }\n",
    "# $( document ).ready(disable_ipyw_close);\n",
    "# </script>\n",
    "\n",
    "# <form action=\"javascript:disable_ipyw_close()\"><input style=\"opacity: 0.5\" type=\"submit\" value=\"Disable ipywidget closing\"></form>'''\n",
    "\n",
    "# wodget_hide = HTML('''<script>\n",
    "# code_show=true; \n",
    "# function code_toggle() {\n",
    "#     if (code_show){\n",
    "#         $('div.cell.code_cell.rendered.selected div.input').hide();\n",
    "#     } else {\n",
    "#         $('div.cell.code_cell.rendered.selected div.input').show();\n",
    "#     }\n",
    "#     code_show = !code_show\n",
    "# } \n",
    "\n",
    "# $( document ).ready(code_toggle);\n",
    "# </script>\n",
    "\n",
    "# To show/hide this cell's raw code input, click <a href=\"javascript:code_toggle()\">here</a>.''')\n",
    "\n",
    "# widget_hide = HTML('''<script>$('div.output_area > div.output_subarea.jupyter-widgets-view > div.p-Widget.p-Panel.jupyter-widgets.widget-container.widget-box.widget-vbox').fadeOut();</script>''')\n",
    "    \n",
    "# def rec_generate(sender):\n",
    "#     if(text_user.value == ''):\n",
    "#         js = \"<script>alert('Enter your username for recommendations!');</script>\"\n",
    "#         display(HTML(js))\n",
    "#         return\n",
    "\n",
    "#     pref_data = pd.read_csv('data/df.csv')\n",
    "#     usr = text_user.value\n",
    "#     user_list = list(pref_data['name'].values)\n",
    "    \n",
    "#     def get_headline(url):\n",
    "#         r = requests.get(url)\n",
    "#         soup = BeautifulSoup(r.text, 'html.parser')\n",
    "#         name_box = soup.findAll('h1')\n",
    "#         return(name_box[0].text)\n",
    "\n",
    "#     def get_snippet(url):\n",
    "#         r = requests.get(url)\n",
    "#         soup = BeautifulSoup(r.text, 'html.parser')\n",
    "#         name_box = soup.findAll('p')\n",
    "#         for i in name_box:\n",
    "#             try:\n",
    "#                 if(len(i.text)>150):\n",
    "#                     snipp = i.text\n",
    "#                     return(snipp)\n",
    "#             except:\n",
    "#                 return(\"No Snippet Available\")\n",
    "\n",
    "#     if(usr in user_list):\n",
    "#         usr_data = pref_data.loc[pref_data['name'] == usr,]\n",
    "#         usr_arr = usr_data.iloc[0].values[1:]\n",
    "# #         usr_arr = usr_data.iloc[0].values[1:]/sum(usr_arr)\n",
    "\n",
    "#         from sklearn.metrics.pairwise import cosine_similarity\n",
    "#         def dot_product(arr1, arr2):\n",
    "#             return(cosine_similarity(arr1.reshape(1,-1),arr2.reshape(1,-1))[0][0])\n",
    "#         score = []\n",
    "#         for i in range(topic_corpus.shape[0]):\n",
    "#             score.append(dot_product(topic_corpus.iloc[i,].values, usr_arr))\n",
    "\n",
    "#         id_score = pd.DataFrame(list(zip(list(content_data['_id']), score)), columns = ['doc_id','lda_sim'])\n",
    "#         doc_data = pd.read_csv('data/user_pref/%s.csv' % usr)\n",
    "#         similarity_df = pd.merge(left = doc_data[['doc_id','cos_sim']], right = id_score, how = 'left', on = 'doc_id')\n",
    "#         similarity_df['age'] = content_data['pub_date'].apply(lambda x: \n",
    "#                                     datetime.today()-datetime.strptime(x[:19], '%Y-%m-%dT%H:%M:%S')).apply(lambda y:\n",
    "#                                     int(y.total_seconds()/3600))\n",
    "#         similarity_df['age_string'] = similarity_df['age'].apply(lambda x: \n",
    "#                                                           str(int(x/24))+' day(s)' if x>24 else str(x)+' hour(s)')\n",
    "#         similarity_df['rank'] = 0.3*similarity_df['cos_sim']+0.7*similarity_df['lda_sim']\n",
    "#         churn_rank = list((1000*similarity_df['rank'])/(np.sqrt(1.00001**similarity_df['age'])))\n",
    "#         sorted_index = sorted(range(len(churn_rank)), key=lambda k: churn_rank[k], reverse = True)[:10]\n",
    "\n",
    "#         url = [content_data['web_url'][i] for i in sorted_index]\n",
    "#         time_stamp = [similarity_df['age_string'][i] for i in sorted_index]\n",
    "#         hdls = [get_headline(i) for i in url]\n",
    "#         snipps = [get_snippet(i) for i in url]\n",
    "#         snipps = ['['+time_stamp[i]+' old] '+snipps[i] for i in range(len(snipps))]\n",
    "#         ui_fill(hdls, snipps, url)\n",
    "\n",
    "#     else:\n",
    "#         base_url = 'https://api.nytimes.com/svc/mostpopular/v2/mostviewed/all-sections/1.json?api-key=c77ddf1d1b594f76b2773928f324615f'\n",
    "#         url = base_url\n",
    "\n",
    "#         r = requests.get(url)\n",
    "#         json_data = r.json()\n",
    "#         article_meta_data = json_data['results']\n",
    "\n",
    "#         headlines = []\n",
    "#         urls = []\n",
    "#         snippets = []\n",
    "#         for artc in article_meta_data[:10]:\n",
    "#             url = artc['url']\n",
    "#             headline = artc['title']\n",
    "#             snippet = artc['abstract']\n",
    "#             headlines.append(headline)\n",
    "#             urls.append(url)\n",
    "#             snippets.append(snippet)\n",
    "            \n",
    "#         ui_fill(headlines, snippets, urls)\n",
    "\n",
    "\n",
    "# def ui_fill(headlines, snippets, urls):\n",
    "    \n",
    "#     display(widget_hide)\n",
    "#     article_topics = query_article_topic(urls)\n",
    "#     article_topics.to_csv('data/waste.csv', index = False)\n",
    "    \n",
    "#     def on_value_change(change):\n",
    "#         if(text_user.value == ''):\n",
    "#             js = \"<script>alert('Enter your name before giving preferences');</script>\"\n",
    "#             display(HTML(js))            \n",
    "#             return\n",
    "        \n",
    "#         # Building user preference based on liked article content\n",
    "#         scale = {'Less':-1, 'Neutral':0, 'More':1}\n",
    "#         pref_data = pd.read_csv('data/df.csv')\n",
    "#         user_list = list(pref_data['name'].values)\n",
    "#         usr = text_user.value\n",
    "        \n",
    "#         if(usr not in user_list):\n",
    "#             # Adding user profile to the topic Similarity Data\n",
    "#             df = pd.DataFrame(columns = ['name'] + [str(i) for i in range(num_topics)])\n",
    "#             df.loc[0] = [usr] + [0 for i in range(num_topics)]\n",
    "#             pref_data = pd.concat([pref_data, df], axis = 0)\n",
    "#             pref_data = pref_data.fillna(0)\n",
    "#             pref_data.to_csv('data/df.csv', index = False)\n",
    "            \n",
    "#             # Adding user profile to the semantic similarity Data\n",
    "#             df_sem = pd.DataFrame(columns = ['doc_id','cos_sim','count'])\n",
    "#             df_sem.to_csv('data/user_pref/%s.csv' %usr, index = False)\n",
    "\n",
    "#         pref_data = pd.read_csv('data/df.csv')\n",
    "#         ind = items_rate.index(change['owner'])\n",
    "#         pref_change = scale[change['new']] - scale[change['old']]               \n",
    "#         for i in range(num_topics):\n",
    "#             pref_data.loc[pref_data['name'] == usr, str(i)] += pref_change*article_topics.loc[ind, i]\n",
    "#         pref_data.to_csv('data/df.csv', index = False)\n",
    "        \n",
    "#         # Building document similarity data utilizing semantics\n",
    "#         docvec = model.infer_vector(pre_process_doc2vec(extract_content(urls[ind])))\n",
    "#         doc_sim = []\n",
    "#         for i in range(len(model.docvecs)):\n",
    "#             x = cosine_similarity(model.docvecs[i].reshape(1,-1), docvec.reshape(1,-1))\n",
    "#             doc_sim.append(x[0][0])\n",
    "#         doc_data = pd.DataFrame(np.column_stack([list(content_data['_id']), doc_sim, \n",
    "#                                                  [1 for i in range(len(model.docvecs))]]), \n",
    "#                                 columns=['doc_id', 'cos_sim', 'count'])\n",
    "\n",
    "#         doc_data_tmp = pd.read_csv('data/user_pref/%s.csv' %usr)\n",
    "#         doc_data = pd.merge(left = doc_data_tmp, right = doc_data, on = 'doc_id', how = 'outer')\n",
    "#         doc_data = doc_data.drop_duplicates('doc_id')\n",
    "#         doc_data = doc_data.loc[pd.notnull(doc_data['doc_id']),]\n",
    "#         doc_data = doc_data.loc[pd.notnull(doc_data['cos_sim_y']),]\n",
    "        \n",
    "#         doc_data['cos_sim'] = np.where(pd.isnull(doc_data['cos_sim_x']) & pd.isnull(doc_data['count_x']), \n",
    "#                                        doc_data['cos_sim_y'], doc_data['cos_sim_x'])\n",
    "#         doc_data['count'] = np.where(pd.isnull(doc_data['cos_sim_x']) & pd.isnull(doc_data['count_x']), \n",
    "#                                        doc_data['count_y'], doc_data['count_x'])\n",
    "        \n",
    "#         doc_data['cos_sim'] = np.where(pd.notnull(doc_data['cos_sim_x']) & pd.notnull(doc_data['count_x']), \n",
    "#                                        (doc_data['cos_sim_x']*doc_data['count_x']+\n",
    "#                                         doc_data.fillna(0)['cos_sim_y'].astype(float)*doc_data.fillna(0)['count_y'].astype(float))/\n",
    "#                                        (doc_data.fillna(0)['count_x'].astype(float) + doc_data.fillna(0)['count_y'].astype(float)),\n",
    "#                                        doc_data['cos_sim'])\n",
    "#         doc_data['count'] = np.where(pd.notnull(doc_data['cos_sim_x']) & pd.notnull(doc_data['count_x']), \n",
    "#                                        doc_data['count_x'].astype(float)+doc_data.fillna(0)['count_y'].astype(float), doc_data['count'])\n",
    "\n",
    "#         doc_data = doc_data.drop(['cos_sim_x','cos_sim_y','count_x','count_y'], axis = 1)\n",
    "#         doc_data.to_csv('data/user_pref/%s.csv' %usr, index = False)\n",
    "\n",
    "    \n",
    "#     def redirect_link(sender):\n",
    "#         ind = items_hl.index(sender)\n",
    "#         link = urls[ind]\n",
    "#         webbrowser.open(link)\n",
    "        \n",
    "#     items_hl = [Button(description=w, border = 'solid', layout = Layout(width='80%', height='40px')) for w in headlines]\n",
    "#     for hl in items_hl:\n",
    "#         hl.style.button_color = 'SkyBlue'\n",
    "#         hl.style.font_weight = 'bold'\n",
    "#         hl.on_click(redirect_link)\n",
    "        \n",
    "#     items_sn = [widgets.Textarea(w, disabled = True, layout = Layout(width = '99.5%', height = '50px')) for w in snippets]\n",
    "\n",
    "#     items_rate = [widgets.SelectionSlider(options=['Less', 'Neutral', 'More'], value = 'Neutral', \n",
    "#                                           description='Rate Article '+str(i+1), disabled=False, \n",
    "#                                           continuous_update=False, orientation='horizontal', readout=True) \n",
    "#                   for i in range(len(headlines))]\n",
    "#     for rt in items_rate:\n",
    "#         rt.observe(on_value_change, names = 'value')\n",
    "\n",
    "#     items = [VBox([HBox([items_hl[i], items_rate[i]]), items_sn[i]]) for i in range(len(headlines))]\n",
    "#     box = VBox(items, layout = Layout(border = 'solid'))\n",
    "#     display(box)\n",
    "\n",
    "    \n",
    "# def onclick(sender):\n",
    "    \n",
    "#     if(text_query.value == ''):\n",
    "#         js = \"<script>alert('Input cannot be blank');</script>\"\n",
    "#         display(HTML(js))\n",
    "#         return\n",
    "    \n",
    "#     base_url = 'https://api.nytimes.com/svc/search/v2/articlesearch.json?api-key=c77ddf1d1b594f76b2773928f324615f'\n",
    "#     param_url = '&q='+str(text_query.value)+'&page=0'\n",
    "#     url = base_url + param_url\n",
    "    \n",
    "#     r = requests.get(url)\n",
    "#     json_data = r.json()\n",
    "#     article_meta_data = json_data['response']['docs']\n",
    "    \n",
    "#     headlines = []\n",
    "#     urls = []\n",
    "#     snippets = []\n",
    "#     for artc in article_meta_data:\n",
    "#         url = artc['web_url']\n",
    "#         headline = artc['headline']['main']\n",
    "#         snippet = artc['snippet']\n",
    "#         headlines.append(headline)\n",
    "#         urls.append(url)\n",
    "#         snippets.append(snippet)\n",
    "\n",
    "#     ui_fill(headlines, snippets, urls)\n",
    "    \n",
    "# text_query = widgets.Text(placeholder='Type a query for NY Times article listing')\n",
    "# button_query = widgets.Button(description = 'Search articles')\n",
    "# query = HBox([text_query, button_query])\n",
    "# query = VBox([Label(\"\"), query, Label(\"\")])\n",
    "# button_query.on_click(onclick)\n",
    "\n",
    "\n",
    "# text_user = widgets.Text(placeholder='Enter your name', value = 'saket')\n",
    "# user_input = HBox([Label(\"Username:\"),text_user])\n",
    "# rec_button = widgets.Button(description = 'Get recommendations', layout = Layout(width = '150px'))\n",
    "# reset_button = widgets.Button(description = 'Reset Preferences')\n",
    "# user = VBox([user_input,HBox([Label(\"->->->->->\"),rec_button,reset_button])])\n",
    "# rec_button.on_click(rec_generate)\n",
    "# reset_button.on_click(reset_pref)\n",
    "\n",
    "# display(HBox([query, Label(\"\"),Label(\"\"), user]))\n",
    "# # HTML(htmlscript_ipywidget_disable_closing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/saketguddeti/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:1059: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.pydata.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"216fde88-73c9-4de5-983c-8f1c895afadf\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  var JS_MIME_TYPE = 'application/javascript';\n",
       "  var HTML_MIME_TYPE = 'text/html';\n",
       "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    var script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    var cell = handle.cell;\n",
       "\n",
       "    var id = cell.output_area._bokeh_element_id;\n",
       "    var server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id !== undefined) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            var element_id = msg.content.text.trim();\n",
       "            Bokeh.index[element_id].model.document.clear();\n",
       "            delete Bokeh.index[element_id];\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    var output_area = handle.output_area;\n",
       "    var output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[0].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      var bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      var script_attrs = bk_div.children[0].attributes;\n",
       "      for (var i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[0].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      var toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[0]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    var el = document.getElementById(\"216fde88-73c9-4de5-983c-8f1c895afadf\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n",
       "    }\n",
       "    finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.info(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(js_urls, callback) {\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = js_urls.length;\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var s = document.createElement('script');\n",
       "      s.src = url;\n",
       "      s.async = false;\n",
       "      s.onreadystatechange = s.onload = function() {\n",
       "        root._bokeh_is_loading--;\n",
       "        if (root._bokeh_is_loading === 0) {\n",
       "          console.log(\"Bokeh: all BokehJS libraries loaded\");\n",
       "          run_callbacks()\n",
       "        }\n",
       "      };\n",
       "      s.onerror = function() {\n",
       "        console.warn(\"failed to load library \" + url);\n",
       "      };\n",
       "      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "    }\n",
       "  };var element = document.getElementById(\"216fde88-73c9-4de5-983c-8f1c895afadf\");\n",
       "  if (element == null) {\n",
       "    console.log(\"Bokeh: ERROR: autoload.js configured with elementid '216fde88-73c9-4de5-983c-8f1c895afadf' but no matching script tag was found. \")\n",
       "    return false;\n",
       "  }\n",
       "\n",
       "  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.14.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.14.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.14.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-0.12.14.min.js\"];\n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    \n",
       "    function(Bokeh) {\n",
       "      \n",
       "    },\n",
       "    function(Bokeh) {\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-0.12.14.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.14.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.14.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.14.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.14.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.14.min.css\");\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if ((root.Bokeh !== undefined) || (force === true)) {\n",
       "      for (var i = 0; i < inline_js.length; i++) {\n",
       "        inline_js[i].call(root, root.Bokeh);\n",
       "      }if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"216fde88-73c9-4de5-983c-8f1c895afadf\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(js_urls, function() {\n",
       "      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"216fde88-73c9-4de5-983c-8f1c895afadf\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n    }\n    finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.info(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(js_urls, callback) {\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = js_urls.length;\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      var s = document.createElement('script');\n      s.src = url;\n      s.async = false;\n      s.onreadystatechange = s.onload = function() {\n        root._bokeh_is_loading--;\n        if (root._bokeh_is_loading === 0) {\n          console.log(\"Bokeh: all BokehJS libraries loaded\");\n          run_callbacks()\n        }\n      };\n      s.onerror = function() {\n        console.warn(\"failed to load library \" + url);\n      };\n      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.getElementsByTagName(\"head\")[0].appendChild(s);\n    }\n  };var element = document.getElementById(\"216fde88-73c9-4de5-983c-8f1c895afadf\");\n  if (element == null) {\n    console.log(\"Bokeh: ERROR: autoload.js configured with elementid '216fde88-73c9-4de5-983c-8f1c895afadf' but no matching script tag was found. \")\n    return false;\n  }\n\n  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.14.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.14.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.14.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-0.12.14.min.js\"];\n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    \n    function(Bokeh) {\n      \n    },\n    function(Bokeh) {\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-0.12.14.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.14.min.css\");\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.14.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.14.min.css\");\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.14.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.14.min.css\");\n    }\n  ];\n\n  function run_inline_js() {\n    \n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (var i = 0; i < inline_js.length; i++) {\n        inline_js[i].call(root, root.Bokeh);\n      }if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"216fde88-73c9-4de5-983c-8f1c895afadf\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(js_urls, function() {\n      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.bokehjs_exec.v0+json": "",
      "text/html": [
       "\n",
       "<script\n",
       "    src=\"http://localhost:57559/autoload.js?bokeh-autoload-element=e2310af5-f3b8-4a4b-b311-7fbeb33641dc&bokeh-absolute-url=http://localhost:57559\"\n",
       "    id=\"e2310af5-f3b8-4a4b-b311-7fbeb33641dc\"\n",
       "    data-bokeh-model-id=\"\"\n",
       "    data-bokeh-doc-id=\"\"\n",
       "></script>"
      ]
     },
     "metadata": {
      "application/vnd.bokehjs_exec.v0+json": {
       "server_id": "9e959bba603543b7b945626d0bc3868f"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from bokeh.io import curdoc, show, output_notebook\n",
    "from bokeh.layouts import column, row\n",
    "from bokeh.models.widgets import TextInput, Button, Paragraph, Div, RadioButtonGroup\n",
    "from bokeh.models import CustomJS\n",
    "from bokeh.application.handlers import FunctionHandler\n",
    "from bokeh.application import Application\n",
    "from bokeh.server.server import Server\n",
    "from tornado.ioloop import IOLoop\n",
    "from functools import partial\n",
    "from datetime import datetime\n",
    "import webbrowser\n",
    "import requests\n",
    "import os\n",
    "\n",
    "def default_recs():\n",
    "    base_url = 'https://api.nytimes.com/svc/mostpopular/v2/mostviewed/all-sections/1.json?api-key=c77ddf1d1b594f76b2773928f324615f'\n",
    "    url = base_url\n",
    "\n",
    "    r = requests.get(url)\n",
    "    json_data = r.json()\n",
    "    article_meta_data = json_data['results']\n",
    "\n",
    "    headlines = []\n",
    "    urls = []\n",
    "    snippets = []\n",
    "    for artc in article_meta_data[:10]:\n",
    "        url = artc['url']\n",
    "        headline = artc['title']\n",
    "        snippet = artc['abstract']\n",
    "        headlines.append(headline)\n",
    "        urls.append(url)\n",
    "        snippets.append(snippet)\n",
    "\n",
    "    article_topics = query_article_topic(urls)\n",
    "    article_topics.to_csv('data/article_topics.csv', index = False)\n",
    "\n",
    "    return(headlines, urls, snippets)\n",
    "\n",
    "headlines_ls, urls_ls, snippets_ls = default_recs()\n",
    "\n",
    "\n",
    "def modify_doc(doc):\n",
    "    \n",
    "    def reset_pref():\n",
    "        usr = text_user.value\n",
    "        df = pd.read_csv('data/df.csv')\n",
    "        df = df.loc[df['name'] != usr,]\n",
    "        df.to_csv('data/df.csv', index = False)\n",
    "        try:\n",
    "            os.remove('data/user_pref/%s.csv' %usr)\n",
    "        except:\n",
    "            None\n",
    "\n",
    "    def rec_generate():\n",
    "        if(text_user.value == ''):\n",
    "            return\n",
    "\n",
    "        bar.text = \"<p style='color:red;font-size:120%;text-align:center'><b>Loading...</b></p>\"\n",
    "        \n",
    "        pref_data = pd.read_csv('data/df.csv')\n",
    "        usr = text_user.value\n",
    "        user_list = list(pref_data['name'].values)\n",
    "\n",
    "        def get_headline(url):\n",
    "            r = requests.get(url)\n",
    "            soup = BeautifulSoup(r.text, 'html.parser')\n",
    "            name_box = soup.findAll('h1')\n",
    "            return(name_box[0].text)\n",
    "\n",
    "        def get_snippet(url):\n",
    "            r = requests.get(url)\n",
    "            soup = BeautifulSoup(r.text, 'html.parser')\n",
    "            name_box = soup.findAll('p')\n",
    "            for i in name_box:\n",
    "                try:\n",
    "                    if(len(i.text)>150):\n",
    "                        snipp = i.text\n",
    "                        return(snipp)\n",
    "                except:\n",
    "                    return(\"No Snippet Available\")\n",
    "\n",
    "        if(usr in user_list):\n",
    "            usr_data = pref_data.loc[pref_data['name'] == usr,]\n",
    "            usr_arr = usr_data.iloc[0].values[1:]\n",
    "#             usr_arr = usr_data.iloc[0].values[1:]/sum(usr_arr)\n",
    "\n",
    "            from sklearn.metrics.pairwise import cosine_similarity\n",
    "            def dot_product(arr1, arr2):\n",
    "                return(cosine_similarity(arr1.reshape(1,-1),arr2.reshape(1,-1))[0][0])\n",
    "            score = []\n",
    "            for i in range(topic_corpus.shape[0]):\n",
    "                score.append(dot_product(topic_corpus.iloc[i,].values, usr_arr))\n",
    "\n",
    "            id_score = pd.DataFrame(list(zip(list(content_data['_id']), score)), columns = ['doc_id','lda_sim'])\n",
    "            doc_data = pd.read_csv('data/user_pref/%s.csv' % usr)\n",
    "            similarity_df = pd.merge(left = doc_data[['doc_id','cos_sim']], right = id_score, how = 'left', on = 'doc_id')\n",
    "            similarity_df['age'] = content_data['pub_date'].apply(lambda x: \n",
    "                                        datetime.today()-datetime.strptime(x[:19], '%Y-%m-%dT%H:%M:%S')).apply(lambda y:\n",
    "                                        int(y.total_seconds()/3600))\n",
    "            similarity_df['age_string'] = similarity_df['age'].apply(lambda x: \n",
    "                                                              str(int(x/24))+' day(s)' if x>24 else str(x)+' hour(s)')\n",
    "            similarity_df['rank'] = 0.3*similarity_df['cos_sim']+0.7*similarity_df['lda_sim']\n",
    "            churn_rank = list((1000*similarity_df['rank'])/(np.sqrt(1.00001**similarity_df['age'])))\n",
    "            sorted_index = sorted(range(len(churn_rank)), key=lambda k: churn_rank[k], reverse = True)[:10]\n",
    "\n",
    "            url = [content_data['web_url'][i] for i in sorted_index]\n",
    "            time_stamp = [similarity_df['age_string'][i] for i in sorted_index]\n",
    "            hdls = [get_headline(i) for i in url]\n",
    "            snipps = [get_snippet(i) for i in url]\n",
    "            snipps = ['['+time_stamp[i]+' old] '+snipps[i] for i in range(len(snipps))]\n",
    "            ui_fill(hdls, snipps, url)\n",
    "        else:\n",
    "            base_url = 'https://api.nytimes.com/svc/mostpopular/v2/mostviewed/all-sections/1.json?api-key=c77ddf1d1b594f76b2773928f324615f'\n",
    "            url = base_url\n",
    "\n",
    "            r = requests.get(url)\n",
    "            json_data = r.json()\n",
    "            article_meta_data = json_data['results']\n",
    "\n",
    "            headlines = []\n",
    "            urls = []\n",
    "            snippets = []\n",
    "            for artc in article_meta_data[:10]:\n",
    "                url = artc['url']\n",
    "                headline = artc['title']\n",
    "                snippet = artc['abstract']\n",
    "                headlines.append(headline)\n",
    "                urls.append(url)\n",
    "                snippets.append(snippet)\n",
    "\n",
    "            ui_fill(headlines, snippets, urls)\n",
    "\n",
    "\n",
    "    def on_value_change(attr, old, new, foo):\n",
    "        if(text_user.value == '' or new == -1):\n",
    "            return\n",
    "        \n",
    "        # Building user preference based on liked article content\n",
    "        scale = {0:-1,-1:0,1:1}\n",
    "        pref_data = pd.read_csv('data/df.csv')\n",
    "        user_list = list(pref_data['name'].values)\n",
    "        usr = text_user.value\n",
    "\n",
    "        if(usr not in user_list):\n",
    "            # Adding user profile to the topic Similarity Data\n",
    "            df = pd.DataFrame(columns = ['name'] + [str(i) for i in range(num_topics)])\n",
    "            df.loc[0] = [usr] + [0 for i in range(num_topics)]\n",
    "            pref_data = pd.concat([pref_data, df], axis = 0)\n",
    "            pref_data = pref_data.fillna(0)\n",
    "            pref_data.to_csv('data/df.csv', index = False)\n",
    "\n",
    "            # Adding user profile to the semantic similarity Data\n",
    "            df_sem = pd.DataFrame(columns = ['doc_id','cos_sim','count'])\n",
    "            df_sem.to_csv('data/user_pref/%s.csv' %usr, index = False)\n",
    "\n",
    "        article_topics = pd.read_csv('data/article_topics.csv')\n",
    "        pref_data = pd.read_csv('data/df.csv')\n",
    "        pref_change = scale[new] - scale[old]\n",
    "#         print(pref_change)\n",
    "        for i in range(num_topics):\n",
    "            pref_data.loc[pref_data['name'] == usr, str(i)] += pref_change*article_topics.iloc[foo, i]\n",
    "        pref_data.to_csv('data/df.csv', index = False)\n",
    "\n",
    "        # Building document similarity data utilizing semantics\n",
    "        docvec = model.infer_vector(pre_process_doc2vec(extract_content(urls_ls[foo])))\n",
    "        doc_sim = []\n",
    "        for i in range(len(model.docvecs)):\n",
    "            x = cosine_similarity(model.docvecs[i].reshape(1,-1), docvec.reshape(1,-1))\n",
    "            doc_sim.append(x[0][0])\n",
    "        doc_data = pd.DataFrame(np.column_stack([list(content_data['_id']), doc_sim, \n",
    "                                                 [1 for i in range(len(model.docvecs))]]), \n",
    "                                columns=['doc_id', 'cos_sim', 'count'])\n",
    "\n",
    "        doc_data_tmp = pd.read_csv('data/user_pref/%s.csv' %usr)\n",
    "        doc_data = pd.merge(left = doc_data_tmp, right = doc_data, on = 'doc_id', how = 'outer')\n",
    "        doc_data = doc_data.drop_duplicates('doc_id')\n",
    "        doc_data = doc_data.loc[pd.notnull(doc_data['doc_id']),]\n",
    "        doc_data = doc_data.loc[pd.notnull(doc_data['cos_sim_y']),]\n",
    "\n",
    "        doc_data['cos_sim'] = np.where(pd.isnull(doc_data['cos_sim_x']) & pd.isnull(doc_data['count_x']), \n",
    "                                       doc_data['cos_sim_y'], doc_data['cos_sim_x'])\n",
    "        doc_data['count'] = np.where(pd.isnull(doc_data['cos_sim_x']) & pd.isnull(doc_data['count_x']), \n",
    "                                       doc_data['count_y'], doc_data['count_x'])\n",
    "\n",
    "        doc_data['cos_sim'] = np.where(pd.notnull(doc_data['cos_sim_x']) & pd.notnull(doc_data['count_x']), \n",
    "                                       (doc_data['cos_sim_x']*doc_data['count_x']+\n",
    "                                        doc_data.fillna(0)['cos_sim_y'].astype(float)*doc_data.fillna(0)['count_y'].astype(float))/\n",
    "                                       (doc_data.fillna(0)['count_x'].astype(float) + doc_data.fillna(0)['count_y'].astype(float)),\n",
    "                                       doc_data['cos_sim'])\n",
    "        doc_data['count'] = np.where(pd.notnull(doc_data['cos_sim_x']) & pd.notnull(doc_data['count_x']), \n",
    "                                       doc_data['count_x'].astype(float)+doc_data.fillna(0)['count_y'].astype(float), doc_data['count'])\n",
    "\n",
    "        doc_data = doc_data.drop(['cos_sim_x','cos_sim_y','count_x','count_y'], axis = 1)\n",
    "        doc_data.to_csv('data/user_pref/%s.csv' %usr, index = False)\n",
    "\n",
    "\n",
    "    def ui_fill(headlines, snippets, urls):\n",
    "    \n",
    "        article_topics = query_article_topic(urls)\n",
    "        article_topics.to_csv('data/article_topics.csv', index = False)\n",
    "        \n",
    "        bar.text = \"<p></p>\"\n",
    "\n",
    "        for i in range(len(headlines)):\n",
    "            items_hl[i].label = headlines[i]\n",
    "            items_sn[i].text = snippets[i]\n",
    "            items_rate[i].active = -1\n",
    "            \n",
    "        global urls_ls\n",
    "        urls_ls = urls\n",
    "\n",
    "\n",
    "    def onclick():\n",
    "    \n",
    "        if(text_query.value == ''):\n",
    "            return\n",
    "        \n",
    "        bar.text = \"<p style='color:red;font-size:120%;text-align:center'><b>Loading...</b></p>\"\n",
    "        \n",
    "        base_url = 'https://api.nytimes.com/svc/search/v2/articlesearch.json?api-key=c77ddf1d1b594f76b2773928f324615f'\n",
    "        param_url = '&q='+str(text_query.value)+'&page=0'\n",
    "        url = base_url + param_url\n",
    "\n",
    "        r = requests.get(url)\n",
    "        json_data = r.json()\n",
    "        article_meta_data = json_data['response']['docs']\n",
    "\n",
    "        headlines = []\n",
    "        urls = []\n",
    "        snippets = []\n",
    "        for artc in article_meta_data:\n",
    "            url = artc['web_url']\n",
    "            headline = artc['headline']['main']\n",
    "            snippet = artc['snippet']\n",
    "            headlines.append(headline)\n",
    "            urls.append(url)\n",
    "            snippets.append(snippet)\n",
    "\n",
    "        ui_fill(headlines, snippets, urls)\n",
    "\n",
    "\n",
    "\n",
    "    output = Paragraph()\n",
    "    text_query = TextInput(value = \"\", width=220, height=10, title = \"Enter article query\")\n",
    "\n",
    "    get_alert2 = CustomJS(args=dict(text_query=text_query), code = \"\"\"\n",
    "\n",
    "    var str = text_query.value;\n",
    "    if (str == \"\"){\n",
    "        alert(\"Input cannot be blank\")\n",
    "    }\n",
    "    \"\"\"\n",
    "                        )\n",
    "\n",
    "    button_query = Button(label = 'Search articles', width=220, \n",
    "                          height=10, button_type='success', callback = get_alert2)\n",
    "    \n",
    "    empty_div = Div(text=\"\", width=70, height=1)\n",
    "    query_grid = row(text_query, column(empty_div, button_query), name = 'query_grid')\n",
    "    \n",
    "    text_user = TextInput(placeholder='Enter your name', value = 'saket', width = 340)\n",
    "    username = Div(text=\"Username:\", width=70, height=10)\n",
    "\n",
    "    get_alert = CustomJS(args=dict(text_user=text_user), code = \"\"\"\n",
    "    \n",
    "    var usr = text_user.value;\n",
    "    var val = cb_obj.active\n",
    "    if (usr == \"\"){\n",
    "        if (val == 1) \n",
    "            {cb_obj.active = -1;} \n",
    "        else \n",
    "            {cb_obj.active = -1;}\n",
    "        alert(\"Enter your name before giving preferences\")\n",
    "    }\"\"\"\n",
    "                        )\n",
    "    \n",
    "    rec_button = Button(label = 'Get recommendations', width=110, \n",
    "                        height=5, button_type='warning', callback = get_alert)\n",
    "    \n",
    "    reset_button = Button(label = 'Reset Preferences', width = 150, \n",
    "                          height = 5, button_type = 'warning', callback = get_alert)\n",
    "    \n",
    "    user_grid1 = row(column(empty_div, username), text_user, name = 'user_grid1')\n",
    "    user_grid2 = row(row(Div(text=\"\", width = 70), rec_button), \n",
    "                     row(Div(text=\"\", width = 5), reset_button),\n",
    "                     name = 'user_grid2')\n",
    "    user_grid = column(user_grid1, user_grid2, name = 'user_grid')\n",
    "    input_grid = row(query_grid, Div(text=\"\", width = 60), user_grid, name = 'input_grid')\n",
    "    listing_grid = empty_div\n",
    "    grid = column(input_grid,\n",
    "                  Div(text=\n",
    "                      \"<p style='color:#808080'>Rate the article 'Less' or 'More' to personalize recommendations</p>\",\n",
    "                        height = 20), \n",
    "                  listing_grid,\n",
    "                  listing_grid,\n",
    "                  name = 'grid')\n",
    "    \n",
    "    items_hl = [Button(label=w, width = 800, height = 40, button_type = 'primary') \n",
    "                       for w in headlines_ls]\n",
    "\n",
    "    items_sn = [Paragraph(text=w, width = 800, height = 61, style = {'background-color':'#F2F3F4'}) \n",
    "                       for w in snippets_ls]\n",
    "\n",
    "    items_rate = [RadioButtonGroup(labels=['Less', 'More'], active = -1, callback = get_alert) \n",
    "                         for i in range(len(headlines_ls))]\n",
    "\n",
    "    items = column([column(row(items_hl[i], items_rate[i]),\n",
    "                            items_sn[i]) for i in range(len(headlines_ls))])\n",
    "        \n",
    "    def redirect_link(foo):\n",
    "#         print(urls_ls[foo])\n",
    "        webbrowser.open_new_tab(urls_ls[foo])\n",
    "\n",
    "    for i, hl in enumerate(items_hl):\n",
    "        hl.on_click(partial(redirect_link, foo = i))\n",
    "\n",
    "    for i, hl in enumerate(items_rate):\n",
    "        hl.on_change('active', partial(on_value_change, foo = i))\n",
    "        \n",
    "    bar = Div(text = \"<p></p>\", height = 20, width = 700)\n",
    "    load_grid = row(Div(text=\"\", width=100, height=1), bar, Div(text=\"\", width=100, height=1))\n",
    "\n",
    "    grid.children[2] = load_grid\n",
    "    grid.children[3] = items\n",
    "    \n",
    "    doc.add_root(grid)\n",
    "    \n",
    "    button_query.on_click(onclick)\n",
    "    rec_button.on_click(rec_generate)\n",
    "    reset_button.on_click(reset_pref)    \n",
    "\n",
    "\n",
    "output_notebook()\n",
    "show(modify_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "1b1b65e519dc471d9f1294c79c4a2f5a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "a25b79c128874b34a37d9518de7af8a4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "a2e97b1e953b408fb50c8dcb12dab61c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "FloatProgressModel",
      "state": {
       "layout": "IPY_MODEL_1b1b65e519dc471d9f1294c79c4a2f5a",
       "max": 1,
       "style": "IPY_MODEL_a25b79c128874b34a37d9518de7af8a4",
       "value": 0.49
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
