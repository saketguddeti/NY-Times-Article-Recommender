{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_content(url):\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    \n",
    "    name_box = soup.findAll('p', attrs={'class': 'story-body-text story-content'})\n",
    "    content = [x.text for x in name_box]\n",
    "    content_final = ' '.join(content)\n",
    "    \n",
    "    if(content_final == ''):\n",
    "        name_box = soup.findAll('p')\n",
    "        content = [x.text for x in name_box]\n",
    "        content_final = ' '.join(content)\n",
    "\n",
    "    return(content_final)\n",
    "\n",
    "import pandas as pd\n",
    "content_data = pd.read_csv('data/content_data.csv')\n",
    "content_data['length'] = content_data['content'].str.strip().str.len()\n",
    "\n",
    "import gensim\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import string\n",
    "\n",
    "def pre_process(text):    \n",
    "    stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "    punctuation = set(string.punctuation)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokenizer = RegexpTokenizer(r'[0-9a-zA-Z]+')\n",
    "\n",
    "    def convert_tag(tag):\n",
    "        \"\"\"\n",
    "        Convert the tag given by nltk.pos_tag to the tag used by wordnet\n",
    "        \"\"\"\n",
    "        tag_dict = {'N': 'n', 'J': 'a', 'R': 'r', 'V': 'v'}\n",
    "        try:\n",
    "            return tag_dict[tag[0]]\n",
    "        except KeyError:\n",
    "            return 'a'\n",
    "\n",
    "    cl_text = (\" \").join(tokenizer.tokenize(text))\n",
    "    cl_text = (\" \").join([s for s in cl_text.lower().split() if s not in stopwords])\n",
    "    cl_text = (\"\").join([s for s in cl_text if s not in punctuation])\n",
    "    cl_text = nltk.word_tokenize(cl_text)\n",
    "    pos = nltk.pos_tag(cl_text)\n",
    "    pos = [convert_tag(t[1]) for t in pos]\n",
    "    cl_text = [lemmatizer.lemmatize(cl_text[i], pos[i]) for i in range(len(cl_text))]\n",
    "    return cl_text\n",
    "\n",
    "content_data['content_clean'] = content_data['content'].apply(pre_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/saketguddeti/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:1059: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "n_features = 1000\n",
    "n_top_words = 20\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.6, min_df=10,\n",
    "                                   max_features=n_features,\n",
    "                                   stop_words='english', \n",
    "                                   token_pattern='(?u)\\\\b\\\\w\\\\w\\\\w+\\\\b')\n",
    "\n",
    "tfidf = tfidf_vectorizer.fit_transform(content_data['content_clean'].apply(lambda x: (\" \").join(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 30\n",
    "\n",
    "nmf = NMF(n_components=num_topics, random_state = 1, alpha = 0.1, l1_ratio = 0.5).fit(tfidf)\n",
    "\n",
    "def topic_corpus():\n",
    "    doctopic = nmf.transform(tfidf)\n",
    "    doctopic = doctopic/np.sum(doctopic, axis = 1, keepdims = True)\n",
    "    doctopic = pd.DataFrame(doctopic)\n",
    "    return(doctopic)\n",
    "\n",
    "topic_corpus = topic_corpus()\n",
    "\n",
    "def query_article_topic(list):\n",
    "    query_content = []\n",
    "    for at in list:\n",
    "        try:\n",
    "            query_content.append(pre_process(extract_content(at)))\n",
    "        except:\n",
    "            query_content.append([''])\n",
    "    \n",
    "    query_content = pd.Series([(\" \").join(x) for x in query_content])\n",
    "    tfidf_query_content = tfidf_vectorizer.transform(query_content)\n",
    "    topic_dist = nmf.transform(tfidf_query_content)\n",
    "    topic_dist = topic_dist#/np.sum(topic_dist, axis = 1, keepdims = True)\n",
    "    topic_dist = pd.DataFrame(topic_dist)\n",
    "    return(topic_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "\n",
    "def pre_process_doc2vec(text):    \n",
    "    stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "    punctuation = set(string.punctuation).union(set(['“','”','—','’','‘']))\n",
    "    punctuation.remove('-')\n",
    "    cl_text = (\" \").join([s for s in text.lower().split() if s not in stopwords])\n",
    "    cl_text = (\"\").join([s for s in cl_text if s not in punctuation])\n",
    "    cl_text = nltk.word_tokenize(cl_text)\n",
    "    return cl_text\n",
    "\n",
    "# class LabeledLineSentence(object):\n",
    "#     def __init__(self, doc_list):\n",
    "#        self.doc_list = doc_list\n",
    "#     def __iter__(self):\n",
    "#         for idx, doc in enumerate(self.doc_list):\n",
    "#             yield LabeledSentence(words = pre_process_doc2vec(doc), tags = [idx])\n",
    "            \n",
    "# it = LabeledLineSentence(list(content_data['content']))\n",
    "\n",
    "# model = Doc2Vec(size=50, min_count=5, alpha=0.025, min_alpha=0.025, workers=8)\n",
    "# model.build_vocab(it)\n",
    "\n",
    "# for epoch in range(10):\n",
    "#     model.train(it, total_examples = model.corpus_count, epochs = model.iter)\n",
    "#     model.alpha -= 0.002  # decrease the learning rate\n",
    "#     model.min_alpha = model.alpha  # fix the learning rate, no decay\n",
    "    \n",
    "# model.save(\"data/doc2vec.model\")\n",
    "model = Doc2Vec.load(\"data/doc2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/saketguddeti/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:1059: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.pydata.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"216fde88-73c9-4de5-983c-8f1c895afadf\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  var JS_MIME_TYPE = 'application/javascript';\n",
       "  var HTML_MIME_TYPE = 'text/html';\n",
       "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    var script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    var cell = handle.cell;\n",
       "\n",
       "    var id = cell.output_area._bokeh_element_id;\n",
       "    var server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id !== undefined) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            var element_id = msg.content.text.trim();\n",
       "            Bokeh.index[element_id].model.document.clear();\n",
       "            delete Bokeh.index[element_id];\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    var output_area = handle.output_area;\n",
       "    var output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[0].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      var bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      var script_attrs = bk_div.children[0].attributes;\n",
       "      for (var i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[0].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      var toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[0]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    var el = document.getElementById(\"216fde88-73c9-4de5-983c-8f1c895afadf\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n",
       "    }\n",
       "    finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.info(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(js_urls, callback) {\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = js_urls.length;\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var s = document.createElement('script');\n",
       "      s.src = url;\n",
       "      s.async = false;\n",
       "      s.onreadystatechange = s.onload = function() {\n",
       "        root._bokeh_is_loading--;\n",
       "        if (root._bokeh_is_loading === 0) {\n",
       "          console.log(\"Bokeh: all BokehJS libraries loaded\");\n",
       "          run_callbacks()\n",
       "        }\n",
       "      };\n",
       "      s.onerror = function() {\n",
       "        console.warn(\"failed to load library \" + url);\n",
       "      };\n",
       "      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "    }\n",
       "  };var element = document.getElementById(\"216fde88-73c9-4de5-983c-8f1c895afadf\");\n",
       "  if (element == null) {\n",
       "    console.log(\"Bokeh: ERROR: autoload.js configured with elementid '216fde88-73c9-4de5-983c-8f1c895afadf' but no matching script tag was found. \")\n",
       "    return false;\n",
       "  }\n",
       "\n",
       "  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.14.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.14.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.14.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-0.12.14.min.js\"];\n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    \n",
       "    function(Bokeh) {\n",
       "      \n",
       "    },\n",
       "    function(Bokeh) {\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-0.12.14.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.14.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.14.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.14.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.14.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.14.min.css\");\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if ((root.Bokeh !== undefined) || (force === true)) {\n",
       "      for (var i = 0; i < inline_js.length; i++) {\n",
       "        inline_js[i].call(root, root.Bokeh);\n",
       "      }if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"216fde88-73c9-4de5-983c-8f1c895afadf\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(js_urls, function() {\n",
       "      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"216fde88-73c9-4de5-983c-8f1c895afadf\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n    }\n    finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.info(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(js_urls, callback) {\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = js_urls.length;\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      var s = document.createElement('script');\n      s.src = url;\n      s.async = false;\n      s.onreadystatechange = s.onload = function() {\n        root._bokeh_is_loading--;\n        if (root._bokeh_is_loading === 0) {\n          console.log(\"Bokeh: all BokehJS libraries loaded\");\n          run_callbacks()\n        }\n      };\n      s.onerror = function() {\n        console.warn(\"failed to load library \" + url);\n      };\n      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.getElementsByTagName(\"head\")[0].appendChild(s);\n    }\n  };var element = document.getElementById(\"216fde88-73c9-4de5-983c-8f1c895afadf\");\n  if (element == null) {\n    console.log(\"Bokeh: ERROR: autoload.js configured with elementid '216fde88-73c9-4de5-983c-8f1c895afadf' but no matching script tag was found. \")\n    return false;\n  }\n\n  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.14.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.14.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.14.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-0.12.14.min.js\"];\n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    \n    function(Bokeh) {\n      \n    },\n    function(Bokeh) {\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-0.12.14.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.14.min.css\");\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.14.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.14.min.css\");\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.14.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.14.min.css\");\n    }\n  ];\n\n  function run_inline_js() {\n    \n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (var i = 0; i < inline_js.length; i++) {\n        inline_js[i].call(root, root.Bokeh);\n      }if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"216fde88-73c9-4de5-983c-8f1c895afadf\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(js_urls, function() {\n      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.bokehjs_exec.v0+json": "",
      "text/html": [
       "\n",
       "<script\n",
       "    src=\"http://localhost:57559/autoload.js?bokeh-autoload-element=e2310af5-f3b8-4a4b-b311-7fbeb33641dc&bokeh-absolute-url=http://localhost:57559\"\n",
       "    id=\"e2310af5-f3b8-4a4b-b311-7fbeb33641dc\"\n",
       "    data-bokeh-model-id=\"\"\n",
       "    data-bokeh-doc-id=\"\"\n",
       "></script>"
      ]
     },
     "metadata": {
      "application/vnd.bokehjs_exec.v0+json": {
       "server_id": "9e959bba603543b7b945626d0bc3868f"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ipywidgets import widgets, HBox, VBox, Layout, Button, Label\n",
    "from IPython.display import display, HTML, clear_output\n",
    "import requests\n",
    "import webbrowser\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def reset_pref(sender):\n",
    "    usr = text_user.value\n",
    "    df = pd.read_csv('data/df.csv')\n",
    "    df = df.loc[df['name'] != usr,]\n",
    "    df.to_csv('data/df.csv', index = False)\n",
    "    try:\n",
    "        os.remove('data/user_pref/%s.csv' %usr)\n",
    "    except:\n",
    "        None\n",
    "\n",
    "htmlscript_ipywidget_disable_closing = '''<script>\n",
    "disable = true\n",
    "function disable_ipyw_close(){\n",
    "    if(disable){\n",
    "        $('div.widget-area > div.prompt > button.close').hide()\n",
    "    }\n",
    "    else{\n",
    "        $('div.widget-area > div.prompt > button.close').show()    \n",
    "    }\n",
    "    disable = !disable\n",
    "}\n",
    "$( document ).ready(disable_ipyw_close);\n",
    "</script>\n",
    "\n",
    "<form action=\"javascript:disable_ipyw_close()\"><input style=\"opacity: 0.5\" type=\"submit\" value=\"Disable ipywidget closing\"></form>'''\n",
    "\n",
    "wodget_hide = HTML('''<script>\n",
    "code_show=true; \n",
    "function code_toggle() {\n",
    "    if (code_show){\n",
    "        $('div.cell.code_cell.rendered.selected div.input').hide();\n",
    "    } else {\n",
    "        $('div.cell.code_cell.rendered.selected div.input').show();\n",
    "    }\n",
    "    code_show = !code_show\n",
    "} \n",
    "\n",
    "$( document ).ready(code_toggle);\n",
    "</script>\n",
    "\n",
    "To show/hide this cell's raw code input, click <a href=\"javascript:code_toggle()\">here</a>.''')\n",
    "\n",
    "widget_hide = HTML('''<script>$('div.output_area > div.output_subarea.jupyter-widgets-view > div.p-Widget.p-Panel.jupyter-widgets.widget-container.widget-box.widget-vbox').fadeOut();</script>''')\n",
    "    \n",
    "def rec_generate(sender):\n",
    "    if(text_user.value == ''):\n",
    "        js = \"<script>alert('Enter your username for recommendations!');</script>\"\n",
    "        display(HTML(js))\n",
    "        return\n",
    "\n",
    "    pref_data = pd.read_csv('data/df.csv')\n",
    "    usr = text_user.value\n",
    "    user_list = list(pref_data['name'].values)\n",
    "    \n",
    "    def get_headline(url):\n",
    "        r = requests.get(url)\n",
    "        soup = BeautifulSoup(r.text, 'html.parser')\n",
    "        name_box = soup.findAll('h1')\n",
    "        return(name_box[0].text)\n",
    "\n",
    "    def get_snippet(url):\n",
    "        r = requests.get(url)\n",
    "        soup = BeautifulSoup(r.text, 'html.parser')\n",
    "        name_box = soup.findAll('p')\n",
    "        for i in name_box:\n",
    "            try:\n",
    "                if(len(i.text)>150):\n",
    "                    snipp = i.text\n",
    "                    return(snipp)\n",
    "            except:\n",
    "                return(\"No Snippet Available\")\n",
    "\n",
    "    if(usr in user_list):\n",
    "        usr_data = pref_data.loc[pref_data['name'] == usr,]\n",
    "        usr_arr = usr_data.iloc[0].values[1:]\n",
    "#         usr_arr = usr_data.iloc[0].values[1:]/sum(usr_arr)\n",
    "\n",
    "        from sklearn.metrics.pairwise import cosine_similarity\n",
    "        def dot_product(arr1, arr2):\n",
    "            return(cosine_similarity(arr1.reshape(1,-1),arr2.reshape(1,-1))[0][0])\n",
    "        score = []\n",
    "        for i in range(topic_corpus.shape[0]):\n",
    "            score.append(dot_product(topic_corpus.iloc[i,].values, usr_arr))\n",
    "\n",
    "        id_score = pd.DataFrame(list(zip(list(content_data['_id']), score)), columns = ['doc_id','lda_sim'])\n",
    "        doc_data = pd.read_csv('data/user_pref/%s.csv' % usr)\n",
    "        similarity_df = pd.merge(left = doc_data[['doc_id','cos_sim']], right = id_score, how = 'left', on = 'doc_id')\n",
    "        similarity_df['age'] = content_data['pub_date'].apply(lambda x: \n",
    "                                    datetime.today()-datetime.strptime(x[:19], '%Y-%m-%dT%H:%M:%S')).apply(lambda y:\n",
    "                                    int(y.total_seconds()/3600))\n",
    "        similarity_df['age_string'] = similarity_df['age'].apply(lambda x: \n",
    "                                                          str(int(x/24))+' day(s)' if x>24 else str(x)+' hour(s)')\n",
    "        similarity_df['rank'] = 0.3*similarity_df['cos_sim']+0.7*similarity_df['lda_sim']\n",
    "        churn_rank = list((1000*similarity_df['rank'])/(np.sqrt(1.00001**similarity_df['age'])))\n",
    "        sorted_index = sorted(range(len(churn_rank)), key=lambda k: churn_rank[k], reverse = True)[:10]\n",
    "\n",
    "        url = [content_data['web_url'][i] for i in sorted_index]\n",
    "        time_stamp = [similarity_df['age_string'][i] for i in sorted_index]\n",
    "        hdls = [get_headline(i) for i in url]\n",
    "        snipps = [get_snippet(i) for i in url]\n",
    "        snipps = ['['+time_stamp[i]+' old] '+snipps[i] for i in range(len(snipps))]\n",
    "        ui_fill(hdls, snipps, url)\n",
    "\n",
    "    else:\n",
    "        base_url = 'https://api.nytimes.com/svc/mostpopular/v2/mostviewed/all-sections/1.json?api-key=c77ddf1d1b594f76b2773928f324615f'\n",
    "        url = base_url\n",
    "\n",
    "        r = requests.get(url)\n",
    "        json_data = r.json()\n",
    "        article_meta_data = json_data['results']\n",
    "\n",
    "        headlines = []\n",
    "        urls = []\n",
    "        snippets = []\n",
    "        for artc in article_meta_data[:10]:\n",
    "            url = artc['url']\n",
    "            headline = artc['title']\n",
    "            snippet = artc['abstract']\n",
    "            headlines.append(headline)\n",
    "            urls.append(url)\n",
    "            snippets.append(snippet)\n",
    "            \n",
    "        ui_fill(headlines, snippets, urls)\n",
    "\n",
    "\n",
    "def ui_fill(headlines, snippets, urls):\n",
    "    \n",
    "    display(widget_hide)\n",
    "    article_topics = query_article_topic(urls)\n",
    "    article_topics.to_csv('data/waste.csv', index = False)\n",
    "    \n",
    "    def on_value_change(change):\n",
    "        if(text_user.value == ''):\n",
    "            js = \"<script>alert('Enter your name before giving preferences');</script>\"\n",
    "            display(HTML(js))            \n",
    "            return\n",
    "        \n",
    "        # Building user preference based on liked article content\n",
    "        scale = {'Less':-1, 'Neutral':0, 'More':1}\n",
    "        pref_data = pd.read_csv('data/df.csv')\n",
    "        user_list = list(pref_data['name'].values)\n",
    "        usr = text_user.value\n",
    "        \n",
    "        if(usr not in user_list):\n",
    "            # Adding user profile to the topic Similarity Data\n",
    "            df = pd.DataFrame(columns = ['name'] + [str(i) for i in range(num_topics)])\n",
    "            df.loc[0] = [usr] + [0 for i in range(num_topics)]\n",
    "            pref_data = pd.concat([pref_data, df], axis = 0)\n",
    "            pref_data = pref_data.fillna(0)\n",
    "            pref_data.to_csv('data/df.csv', index = False)\n",
    "            \n",
    "            # Adding user profile to the semantic similarity Data\n",
    "            df_sem = pd.DataFrame(columns = ['doc_id','cos_sim','count'])\n",
    "            df_sem.to_csv('data/user_pref/%s.csv' %usr, index = False)\n",
    "\n",
    "        pref_data = pd.read_csv('data/df.csv')\n",
    "        ind = items_rate.index(change['owner'])\n",
    "        pref_change = scale[change['new']] - scale[change['old']]               \n",
    "        for i in range(num_topics):\n",
    "            pref_data.loc[pref_data['name'] == usr, str(i)] += pref_change*article_topics.loc[ind, i]\n",
    "        pref_data.to_csv('data/df.csv', index = False)\n",
    "        \n",
    "        # Building document similarity data utilizing semantics\n",
    "        docvec = model.infer_vector(pre_process_doc2vec(extract_content(urls[ind])))\n",
    "        doc_sim = []\n",
    "        for i in range(len(model.docvecs)):\n",
    "            x = cosine_similarity(model.docvecs[i].reshape(1,-1), docvec.reshape(1,-1))\n",
    "            doc_sim.append(x[0][0])\n",
    "        doc_data = pd.DataFrame(np.column_stack([list(content_data['_id']), doc_sim, \n",
    "                                                 [1 for i in range(len(model.docvecs))]]), \n",
    "                                columns=['doc_id', 'cos_sim', 'count'])\n",
    "\n",
    "        doc_data_tmp = pd.read_csv('data/user_pref/%s.csv' %usr)\n",
    "        doc_data = pd.merge(left = doc_data_tmp, right = doc_data, on = 'doc_id', how = 'outer')\n",
    "        doc_data = doc_data.drop_duplicates('doc_id')\n",
    "        doc_data = doc_data.loc[pd.notnull(doc_data['doc_id']),]\n",
    "        doc_data = doc_data.loc[pd.notnull(doc_data['cos_sim_y']),]\n",
    "        \n",
    "        doc_data['cos_sim'] = np.where(pd.isnull(doc_data['cos_sim_x']) & pd.isnull(doc_data['count_x']), \n",
    "                                       doc_data['cos_sim_y'], doc_data['cos_sim_x'])\n",
    "        doc_data['count'] = np.where(pd.isnull(doc_data['cos_sim_x']) & pd.isnull(doc_data['count_x']), \n",
    "                                       doc_data['count_y'], doc_data['count_x'])\n",
    "        \n",
    "        doc_data['cos_sim'] = np.where(pd.notnull(doc_data['cos_sim_x']) & pd.notnull(doc_data['count_x']), \n",
    "                                       (doc_data['cos_sim_x']*doc_data['count_x']+\n",
    "                                        doc_data.fillna(0)['cos_sim_y'].astype(float)*doc_data.fillna(0)['count_y'].astype(float))/\n",
    "                                       (doc_data.fillna(0)['count_x'].astype(float) + doc_data.fillna(0)['count_y'].astype(float)),\n",
    "                                       doc_data['cos_sim'])\n",
    "        doc_data['count'] = np.where(pd.notnull(doc_data['cos_sim_x']) & pd.notnull(doc_data['count_x']), \n",
    "                                       doc_data['count_x'].astype(float)+doc_data.fillna(0)['count_y'].astype(float), doc_data['count'])\n",
    "\n",
    "        doc_data = doc_data.drop(['cos_sim_x','cos_sim_y','count_x','count_y'], axis = 1)\n",
    "        doc_data.to_csv('data/user_pref/%s.csv' %usr, index = False)\n",
    "\n",
    "    \n",
    "    def redirect_link(sender):\n",
    "        ind = items_hl.index(sender)\n",
    "        link = urls[ind]\n",
    "        webbrowser.open(link)\n",
    "        \n",
    "    items_hl = [Button(description=w, border = 'solid', layout = Layout(width='80%', height='40px')) for w in headlines]\n",
    "    for hl in items_hl:\n",
    "        hl.style.button_color = 'SkyBlue'\n",
    "        hl.style.font_weight = 'bold'\n",
    "        hl.on_click(redirect_link)\n",
    "        \n",
    "    items_sn = [widgets.Textarea(w, disabled = True, layout = Layout(width = '99.5%', height = '50px')) for w in snippets]\n",
    "\n",
    "    items_rate = [widgets.SelectionSlider(options=['Less', 'Neutral', 'More'], value = 'Neutral', \n",
    "                                          description='Rate Article '+str(i+1), disabled=False, \n",
    "                                          continuous_update=False, orientation='horizontal', readout=True) \n",
    "                  for i in range(len(headlines))]\n",
    "    for rt in items_rate:\n",
    "        rt.observe(on_value_change, names = 'value')\n",
    "\n",
    "    items = [VBox([HBox([items_hl[i], items_rate[i]]), items_sn[i]]) for i in range(len(headlines))]\n",
    "    box = VBox(items, layout = Layout(border = 'solid'))\n",
    "    display(box)\n",
    "\n",
    "    \n",
    "def onclick(sender):\n",
    "    \n",
    "    if(text_query.value == ''):\n",
    "        js = \"<script>alert('Input cannot be blank');</script>\"\n",
    "        display(HTML(js))\n",
    "        return\n",
    "    \n",
    "    base_url = 'https://api.nytimes.com/svc/search/v2/articlesearch.json?api-key=c77ddf1d1b594f76b2773928f324615f'\n",
    "    param_url = '&q='+str(text_query.value)+'&page=0'\n",
    "    url = base_url + param_url\n",
    "    \n",
    "    r = requests.get(url)\n",
    "    json_data = r.json()\n",
    "    article_meta_data = json_data['response']['docs']\n",
    "    \n",
    "    headlines = []\n",
    "    urls = []\n",
    "    snippets = []\n",
    "    for artc in article_meta_data:\n",
    "        url = artc['web_url']\n",
    "        headline = artc['headline']['main']\n",
    "        snippet = artc['snippet']\n",
    "        headlines.append(headline)\n",
    "        urls.append(url)\n",
    "        snippets.append(snippet)\n",
    "\n",
    "    ui_fill(headlines, snippets, urls)\n",
    "    \n",
    "text_query = widgets.Text(placeholder='Type a query for NY Times article listing')\n",
    "button_query = widgets.Button(description = 'Search articles')\n",
    "query = HBox([text_query, button_query])\n",
    "query = VBox([Label(\"\"), query, Label(\"\")])\n",
    "button_query.on_click(onclick)\n",
    "\n",
    "\n",
    "text_user = widgets.Text(placeholder='Enter your name', value = 'saket')\n",
    "user_input = HBox([Label(\"Username:\"),text_user])\n",
    "rec_button = widgets.Button(description = 'Get recommendations', layout = Layout(width = '150px'))\n",
    "reset_button = widgets.Button(description = 'Reset Preferences')\n",
    "user = VBox([user_input,HBox([Label(\"->->->->->\"),rec_button,reset_button])])\n",
    "rec_button.on_click(rec_generate)\n",
    "reset_button.on_click(reset_pref)\n",
    "\n",
    "display(HBox([query, Label(\"\"),Label(\"\"), user]))\n",
    "# HTML(htmlscript_ipywidget_disable_closing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "1b1b65e519dc471d9f1294c79c4a2f5a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "a25b79c128874b34a37d9518de7af8a4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "a2e97b1e953b408fb50c8dcb12dab61c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "FloatProgressModel",
      "state": {
       "layout": "IPY_MODEL_1b1b65e519dc471d9f1294c79c4a2f5a",
       "max": 1,
       "style": "IPY_MODEL_a25b79c128874b34a37d9518de7af8a4",
       "value": 0.49
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
